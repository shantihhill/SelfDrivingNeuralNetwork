{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SHANTIH MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CODE TO START NORMALIZING TEST AND RUN ALL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_data's shape (10000, 50, 110, 6)\n",
            "test_data's shape (2100, 50, 50, 6)\n"
          ]
        }
      ],
      "source": [
        "train_file = np.load('./cse-251-b-2025/train.npz')\n",
        "\n",
        "train_data = train_file['data']\n",
        "print(\"train_data's shape\", train_data.shape)\n",
        "test_file = np.load('./cse-251-b-2025/test_input.npz')\n",
        "\n",
        "test_data = test_file['data']\n",
        "print(\"test_data's shape\", test_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2100, 50, 2)\n"
          ]
        }
      ],
      "source": [
        "test_data = test_file['data']\n",
        "test_x = test_data[:, 0, :50, :2]\n",
        "initial_test_x = test_x[:, 0:1, :].copy()\n",
        "test_x -= initial_test_x\n",
        "\n",
        "print(test_x.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Code for autoregressive stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CHANGE THESE FOR THE NUMBER OF TIMESTEPS TO USE AND THE NUMBER OF TIMESTEPS AHEAD TO PREDICT\n",
        "num_features = 50\n",
        "num_labels = 10\n",
        "\n",
        "def do_autoregressive(train_data, num_features, num_labels):\n",
        "    ar_train_x, ar_train_y = [], []\n",
        "\n",
        "    for s in range(train_data.shape[0]):\n",
        "        for p in range(num_features, 110 - num_labels):\n",
        "            train_x, train_y = train_data[s, 0, p-num_features:p, :2], train_data[s, 0, p:p+num_labels, :2] \n",
        "            initial_train_x, = train_x[0:1, :].copy()\n",
        "            train_x -= initial_train_x\n",
        "            train_y -= initial_train_x\n",
        "            ar_train_x.append(train_x)\n",
        "            ar_train_y.append(train_y)\n",
        "    ar_train_x = np.stack(ar_train_x, axis=0)\n",
        "    ar_train_y = np.stack(ar_train_y, axis=0)\n",
        "    return ar_train_x, ar_train_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10000, 50, 2)\n",
            "(10000, 50, 2) (10000, 60, 2)\n",
            "(8000, 50, 2) (8000, 60, 2)\n",
            "(2000, 50, 2) (2000, 60, 2)\n"
          ]
        }
      ],
      "source": [
        "#SET TO FALSE IF YOU DONT WANT AUTOREGRESSIVE\n",
        "#auto_regressive = False\n",
        "auto_regressive = True\n",
        "\n",
        "train_x, train_y = train_data[:, 0, :50, :2], train_data[:, 0, 50:, :2]\n",
        "initial_train_x, initial_train_y = train_x[:, 0:1, :].copy(), train_y[:, 0:1, :].copy()\n",
        "train_x -= initial_train_x\n",
        "train_y -= initial_train_x\n",
        "\n",
        "print(train_x.shape)\n",
        "\n",
        "if auto_regressive:\n",
        "    train_x, train_y = do_autoregressive(train_data, num_features, num_labels)\n",
        "    print(train_x.shape, train_y.shape)\n",
        "\n",
        "ratio_validation = 0.2\n",
        "perm = torch.randperm(train_x.shape[0])\n",
        "idx = int(ratio_validation * train_x.shape[0])\n",
        "\n",
        "new_train_x = train_x[perm[idx:]]\n",
        "new_train_y = train_y[perm[idx:]]\n",
        "\n",
        "val_x = train_x[perm[:idx]]\n",
        "val_y = train_y[perm[:idx]]\n",
        "\n",
        "print(train_x.shape, train_y.shape)\n",
        "print(new_train_x.shape, new_train_y.shape)\n",
        "print(val_x.shape, val_y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Code for adding closest agent feature (in progress)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dist(x1, y1, x2, y2):\n",
        "    return Math.sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2)\n",
        "\n",
        "def add_closest_agent(x):\n",
        "    #x is the features.\n",
        "    #I want to take them from (num_scenes, num_agents, num_timesteps, num_dimensions) to (num_scenes, num_timesteps, num_dimensions * 2) by adding the dimensions of the closest agent\n",
        "\n",
        "    new_x = []\n",
        "    for s in x.shape[0]:\n",
        "        each_scene = []\n",
        "        for t in x.shape[2]:\n",
        "            ca = 1\n",
        "            for a in x.shape[1]:\n",
        "                if dist(x[s][0][t][0], x[s][0][t][1], x[s][a][t][0], x[s][a][t][1]) < dist(x[s][0][t][0], x[s][0][t][1], x[s][ca][t][0], x[s][ca][t][1]):\n",
        "                    ca = a\n",
        "            each_scene.append([x[s][0][t][0], x[s][0][t][1], x[s][ca][t][0], x[s][ca][t][1]])\n",
        "        new_x.append(each_scene)\n",
        "\n",
        "    return new_x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Play with the number of layers and sizes. My computer takes too long"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ComplexMLP(nn.Module):\n",
        "    def __init__(self, input_features, output_features):\n",
        "        super(ComplexMLP, self).__init__()\n",
        "\n",
        "        # Define the layers\n",
        "        self.input_features = input_features\n",
        "        self.output_features = output_features\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_features, 1024),\n",
        "            nn.ReLU(),\n",
        "            #nn.Dropout(0.1),\n",
        "\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            #nn.Dropout(0.1),\n",
        "\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            #nn.Dropout(0.1),\n",
        "\n",
        "            nn.Linear(256, output_features)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.mlp(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(model, x):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x_tensor = torch.FloatTensor(x).reshape((-1, input_features))\n",
        "        predictions = model(x_tensor).reshape((-1, int(output_features / 2), 2))\n",
        "        return predictions.numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, x, y, dataset):\n",
        "    pred_y = predict(model, x)\n",
        "        \n",
        "    mse = ((pred_y - y) ** 2).mean()\n",
        "    print(\"Model MSE evaluated on\", dataset, \":\", mse.item())\n",
        "    return mse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example of how to prepare data and train the model\n",
        "\n",
        "def new_train_model(model, criterion, optimizer, train_x, train_y, val_x, val_y, batch_size=64, epochs=10):\n",
        "    # Convert numpy arrays to PyTorch tensors\n",
        "    X_train_tensor = torch.FloatTensor(train_x).reshape((-1, model.input_features))\n",
        "    y_train_tensor = torch.FloatTensor(train_y).reshape((-1, model.output_features))\n",
        "    print(X_train_tensor.shape)\n",
        "    print(y_train_tensor.shape)\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(batch_X)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, batch_y)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Print epoch statistics\n",
        "        #print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}')\n",
        "        print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}')\n",
        "        evaluate_model(model, train_x, train_y, \"TRAIN\")\n",
        "        evaluate_model(model, val_x, val_y, \"VALIDATION\")\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run this to train the model. You can interrupt it if it takes too long and continue on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100 120\n",
            "torch.Size([8000, 100])\n",
            "torch.Size([8000, 120])\n",
            "Epoch 1, Loss: 157.1762\n",
            "Model MSE evaluated on TRAIN : 53.42602901237716\n",
            "Model MSE evaluated on VALIDATION : 52.9545423526227\n",
            "Epoch 2, Loss: 43.9422\n",
            "Model MSE evaluated on TRAIN : 45.00088687284408\n",
            "Model MSE evaluated on VALIDATION : 44.3307604812709\n",
            "Epoch 3, Loss: 39.3458\n",
            "Model MSE evaluated on TRAIN : 38.906961109990895\n",
            "Model MSE evaluated on VALIDATION : 39.004875587928694\n",
            "Epoch 4, Loss: 37.9428\n",
            "Model MSE evaluated on TRAIN : 32.87745021870372\n",
            "Model MSE evaluated on VALIDATION : 33.46889555617869\n",
            "Epoch 5, Loss: 36.5004\n",
            "Model MSE evaluated on TRAIN : 31.960838469630744\n",
            "Model MSE evaluated on VALIDATION : 32.28646478842156\n",
            "Epoch 6, Loss: 33.1266\n",
            "Model MSE evaluated on TRAIN : 35.57486355569097\n",
            "Model MSE evaluated on VALIDATION : 35.776053653390015\n",
            "Epoch 7, Loss: 32.3264\n",
            "Model MSE evaluated on TRAIN : 37.91527700048772\n",
            "Model MSE evaluated on VALIDATION : 38.03283982450787\n",
            "Epoch 8, Loss: 34.5367\n",
            "Model MSE evaluated on TRAIN : 30.202733187216978\n",
            "Model MSE evaluated on VALIDATION : 30.61692799442088\n",
            "Epoch 9, Loss: 31.7372\n",
            "Model MSE evaluated on TRAIN : 38.57428622065825\n",
            "Model MSE evaluated on VALIDATION : 38.79478497160523\n",
            "Epoch 10, Loss: 30.1673\n",
            "Model MSE evaluated on TRAIN : 27.041340178607598\n",
            "Model MSE evaluated on VALIDATION : 27.162371413149554\n",
            "Epoch 11, Loss: 27.9570\n",
            "Model MSE evaluated on TRAIN : 27.894211520084884\n",
            "Model MSE evaluated on VALIDATION : 27.81398119705101\n",
            "Epoch 12, Loss: 29.1165\n",
            "Model MSE evaluated on TRAIN : 26.73658045643269\n",
            "Model MSE evaluated on VALIDATION : 26.799102666526643\n",
            "Epoch 13, Loss: 28.3487\n",
            "Model MSE evaluated on TRAIN : 25.795626547712196\n",
            "Model MSE evaluated on VALIDATION : 26.18932394146742\n",
            "Epoch 14, Loss: 28.5871\n",
            "Model MSE evaluated on TRAIN : 30.427981035048564\n",
            "Model MSE evaluated on VALIDATION : 30.623027173722274\n",
            "Epoch 15, Loss: 28.3387\n",
            "Model MSE evaluated on TRAIN : 27.893195863426516\n",
            "Model MSE evaluated on VALIDATION : 28.009816672775884\n",
            "Epoch 16, Loss: 27.6629\n",
            "Model MSE evaluated on TRAIN : 34.27807807028977\n",
            "Model MSE evaluated on VALIDATION : 34.12441121116223\n",
            "Epoch 17, Loss: 27.4379\n",
            "Model MSE evaluated on TRAIN : 24.411755235094656\n",
            "Model MSE evaluated on VALIDATION : 24.47892312760268\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     13\u001b[39m criterion = nn.MSELoss()\n\u001b[32m     15\u001b[39m optimizer = optim.Adam(norm_model.parameters(), lr=\u001b[32m0.001\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m norm_model = \u001b[43mnew_train_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_train_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_train_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mnew_train_model\u001b[39m\u001b[34m(model, criterion, optimizer, train_x, train_y, val_x, val_y, batch_size, epochs)\u001b[39m\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[32m     29\u001b[39m     loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m     running_loss += loss.item()\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Print epoch statistics\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m#print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}')\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cse251/lib/python3.12/site-packages/torch/optim/optimizer.py:385\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    380\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    381\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    382\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    383\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    388\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cse251/lib/python3.12/site-packages/torch/optim/optimizer.py:76\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     74\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m'\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     75\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cse251/lib/python3.12/site-packages/torch/optim/adam.py:166\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    155\u001b[39m     beta1, beta2 = group[\u001b[33m'\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    157\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    158\u001b[39m         group,\n\u001b[32m    159\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    163\u001b[39m         max_exp_avg_sqs,\n\u001b[32m    164\u001b[39m         state_steps)\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cse251/lib/python3.12/site-packages/torch/optim/adam.py:316\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    314\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cse251/lib/python3.12/site-packages/torch/optim/adam.py:439\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[39m\n\u001b[32m    437\u001b[39m         denom = (max_exp_avg_sqs[i].sqrt() / bias_correction2_sqrt).add_(eps)\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m         denom = \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m     param.addcdiv_(exp_avg, denom, value=-step_size)\n\u001b[32m    443\u001b[39m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "input_features = 50 * 2  # = 100\n",
        "output_features = 60 * 2\n",
        "\n",
        "if auto_regressive:\n",
        "    input_features = num_features * 2\n",
        "    output_features = num_labels * 2\n",
        "\n",
        "print(input_features, output_features)\n",
        "\n",
        "norm_model = ComplexMLP(input_features, output_features)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "optimizer = optim.Adam(norm_model.parameters(), lr=0.001)\n",
        "\n",
        "norm_model = new_train_model(norm_model, criterion, optimizer, new_train_x, new_train_y, val_x, val_y, batch_size=64, epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100 20 10.0\n",
            "Model MSE evaluated on TRAIN : 0.3644307232142692\n",
            "100 20 10.0\n",
            "Model MSE evaluated on VALIDATION : 0.36405780795426457\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.36405780795426457"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_model(norm_model, train_x, train_y, \"TRAIN\")\n",
        "evaluate_model(norm_model, val_x, val_y, \"VALIDATION\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Need this method for autoregressive inference. Basically doing and shifting thing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ar_inference(norm_model, feature):\n",
        "    print('in', feature.shape)\n",
        "    answer = []\n",
        "    for i in range(int(60 / num_labels)):\n",
        "        pred = predict(norm_model, feature)\n",
        "        answer.append(pred)\n",
        "        feature = np.concatenate((feature, pred), axis=1)\n",
        "        feature = feature[:, -num_features:, :]\n",
        "    answer = np.concatenate(answer, axis=1)\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remember you need to delete the shantih_mlp.csv if you want to make a new one. I don't think it rewrites it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2100, 50, 2)\n",
            "in (2100, 50, 2)\n",
            "100 20 10.0\n",
            "100 20 10.0\n",
            "100 20 10.0\n",
            "100 20 10.0\n",
            "100 20 10.0\n",
            "100 20 10.0\n"
          ]
        }
      ],
      "source": [
        "print(test_x.shape)\n",
        "if not auto_regressive:\n",
        "    pred_y = predict(norm_model, test_x)\n",
        "else:\n",
        "    pred_y = ar_inference(norm_model, test_x)\n",
        "\n",
        "pred_y += initial_test_x\n",
        "\n",
        "# Code to write the prediction to file\n",
        "pred_output = pred_y.reshape(-1, 2)\n",
        "output_df = pd.DataFrame(pred_output, columns=['x', 'y'])\n",
        "\n",
        "output_df.index.name = 'index'\n",
        "\n",
        "output_df.to_csv('shantih_mlp.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "data-loading-and-submission-preperation",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30918,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "cse251",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 27.524611,
      "end_time": "2025-04-01T17:39:42.223757",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-04-01T17:39:14.699146",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
