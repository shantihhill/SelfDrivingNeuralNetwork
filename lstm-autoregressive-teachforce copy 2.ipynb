{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-n-C_P-zs-pY"
      },
      "source": [
        "# LSTM - vanilla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model = \"best_model_all_feat_2layers_all_agents.pt\"\n",
        "\n",
        "# best_model = \"best_model_all_feat_all_agents.pt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'best_model_all_feat_2layers_all_agents.pt'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2025-04-01T17:39:17.728787Z",
          "iopub.status.busy": "2025-04-01T17:39:17.728324Z",
          "iopub.status.idle": "2025-04-01T17:39:18.875979Z",
          "shell.execute_reply": "2025-04-01T17:39:18.874618Z"
        },
        "id": "F4_wHjhZs-pa",
        "papermill": {
          "duration": 1.154057,
          "end_time": "2025-04-01T17:39:18.878059",
          "exception": false,
          "start_time": "2025-04-01T17:39:17.724002",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch_geometric.data import Data, Batch\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T17:39:18.890746Z",
          "iopub.status.busy": "2025-04-01T17:39:18.890075Z",
          "iopub.status.idle": "2025-04-01T17:39:40.968717Z",
          "shell.execute_reply": "2025-04-01T17:39:40.967158Z"
        },
        "id": "09texa_os-pc",
        "papermill": {
          "duration": 22.084222,
          "end_time": "2025-04-01T17:39:40.970631",
          "exception": false,
          "start_time": "2025-04-01T17:39:18.886409",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_data's shape (10000, 50, 110, 6)\n",
            "test_data's shape (2100, 50, 50, 6)\n"
          ]
        }
      ],
      "source": [
        "train_file = np.load('../cse-251-b-2025/train.npz')\n",
        "\n",
        "train_data = train_file['data']\n",
        "# train_data = train_data[::2]\n",
        "print(\"train_data's shape\", train_data.shape)\n",
        "test_file = np.load('../cse-251-b-2025/test_input.npz')\n",
        "\n",
        "test_data = test_file['data']\n",
        "print(\"test_data's shape\", test_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T17:39:40.97884Z",
          "iopub.status.busy": "2025-04-01T17:39:40.978362Z",
          "iopub.status.idle": "2025-04-01T17:39:41.323077Z",
          "shell.execute_reply": "2025-04-01T17:39:41.321787Z"
        },
        "id": "ckJbGP_ds-pc",
        "papermill": {
          "duration": 0.351374,
          "end_time": "2025-04-01T17:39:41.325147",
          "exception": false,
          "start_time": "2025-04-01T17:39:40.973773",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plot one\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data_matrix = train_data[0]\n",
        "\n",
        "for i in range(data_matrix.shape[0]):\n",
        "    xs = data_matrix[i, :, 0]\n",
        "    ys = data_matrix[i, :, 1]\n",
        "    # trim all zeros\n",
        "    xs = xs[xs != 0]\n",
        "    ys = ys[ys != 0]\n",
        "    # plot each line going from transparent to full\n",
        "    plt.plot(xs, ys)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TrajectoryDatasetTrain(Dataset):\n",
        "    def __init__(self, data, scale=10.0, augment=True):\n",
        "        \"\"\"\n",
        "        data: Shape (N, 50, 110, 6) Training data\n",
        "        scale: Scale for normalization (suggested to use 10.0 for Argoverse 2 data)\n",
        "        augment: Whether to apply data augmentation (only for training)\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.scale = scale\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        scene = self.data[idx]\n",
        "        # Getting 50 historical timestamps and 60 future timestamps\n",
        "        hist = scene[:, :50, :5].copy()    # (agents=50, time_seq=50, 6)\n",
        "        future = torch.tensor(scene[0, 50:, :2].copy(), dtype=torch.float32)  # (50, 60, 2)\n",
        "        #add the feature of the scene number for each sample\n",
        "       \n",
        " \n",
        "        # Data augmentation(only for training)\n",
        "        if self.augment:\n",
        "            if np.random.rand() < 0.5:\n",
        "                theta = np.random.uniform(-np.pi, np.pi)\n",
        "                R = np.array([[np.cos(theta), -np.sin(theta)],\n",
        "                              [np.sin(theta),  np.cos(theta)]], dtype=np.float32)\n",
        "                # Rotate the historical trajectory and future trajectory\n",
        "                hist[..., :2] = hist[..., :2] @ R\n",
        "                hist[..., 2:4] = hist[..., 2:4] @ R\n",
        "                future = future @ R\n",
        "                # future[..., 2:4] = future[..., 2:4] @ R\n",
        "            if np.random.rand() < 0.5:\n",
        "                hist[..., 0] *= -1\n",
        "                hist[..., 2] *= -1\n",
        "                future[:, 0] *= -1\n",
        "                # future[:, 2] *= -1\n",
        "\n",
        "        # Use the last timeframe of the historical trajectory as the origin\n",
        "        origin = hist[0, 49, :2].copy()  # (2,)\n",
        "        hist[..., :2] = hist[..., :2] - origin\n",
        "        # future[..., :2] = future[..., :2] - origin\n",
        "        future = future - origin\n",
        "\n",
        "        # Normalize the historical trajectory and future trajectory\n",
        "        hist[..., :4] = hist[..., :4] / self.scale\n",
        "        future = future / self.scale\n",
        "        # hist[..., :2] = hist[..., :2] / self.scale\n",
        "        # future[..., :2] = future[..., :2] / self.scale\n",
        "\n",
        "        \n",
        "        # print(\"hist's shape\", hist.shape)\n",
        "        data_item = Data(\n",
        "            x=torch.tensor(hist, dtype=torch.float32),\n",
        "            y=future.type(torch.float32),\n",
        "            origin=torch.tensor(origin, dtype=torch.float32).unsqueeze(0), # (1,2)\n",
        "            scale=torch.tensor(self.scale, dtype=torch.float32), # scalar e.g. 7.0\n",
        "        )\n",
        "\n",
        "        return data_item\n",
        "    \n",
        "\n",
        "class TrajectoryDatasetTest(Dataset):\n",
        "    def __init__(self, data, scale=10.0):\n",
        "        \"\"\"\n",
        "        data: Shape (N, 50, 110, 6) Testing data\n",
        "        scale: Scale for normalization (suggested to use 10.0 for Argoverse 2 data)\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.scale = scale\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Testing data only contains historical trajectory\n",
        "        scene = self.data[idx]  # (50, 50, 6)\n",
        "        hist = scene.copy()\n",
        "        # hist = hist[...,]\n",
        "        \n",
        "        origin = hist[0, 49, :2].copy()\n",
        "        hist[..., :2] = hist[..., :2] - origin\n",
        "        hist[..., :4] = hist[..., :4] / self.scale\n",
        "        hist[..., :2] = hist[..., :2] / self.scale\n",
        "\n",
        "        data_item = Data(\n",
        "            x=torch.tensor(hist, dtype=torch.float32),\n",
        "            origin=torch.tensor(origin, dtype=torch.float32).unsqueeze(0),\n",
        "            scale=torch.tensor(self.scale, dtype=torch.float32),\n",
        "        )\n",
        "        return data_item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Apple Silicon GPU\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(251)\n",
        "np.random.seed(42)\n",
        "\n",
        "scale = 7.0 #why not 10\n",
        "\n",
        "N = len(train_data)\n",
        "val_size = int(0.1 * N)\n",
        "train_size = N - val_size\n",
        "\n",
        "train_dataset = TrajectoryDatasetTrain(train_data[:train_size], scale=scale, augment=True)\n",
        "val_dataset = TrajectoryDatasetTrain(train_data[train_size:], scale=scale, augment=False)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=lambda x: Batch.from_data_list(x))\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=lambda x: Batch.from_data_list(x))\n",
        "\n",
        "# Set device for training speedup\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device('mps')\n",
        "    print(\"Using Apple Silicon GPU\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print(\"Using CUDA GPU\")\n",
        "else:\n",
        "    device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AutoRegressiveLSTM(nn.Module):\n",
        "    def __init__(self, input_dim=5, hidden_dim=512, output_dim=2, num_layers=1, future_steps=60):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.future_steps = future_steps\n",
        "\n",
        "        # Encoder: takes in past trajectory\n",
        "        self.encoder = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "\n",
        "        # Decoder: predicts future positions one step at a time\n",
        "        self.decoder = nn.LSTM(input_size=output_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, data, forcing_ratio = 0.5):\n",
        "    # def forward(self, data):\n",
        "        x = data.x[..., :self.input_dim] \n",
        "       \n",
        "        x = x.reshape(-1, 50, 50, self.input_dim)[:, 0, :, :]  # (B*A, 50, 5)\n",
        "        # x = x.reshape(-1, 50, self.input_dim)\n",
        "        batch_size = x.size(0)\n",
        "        \n",
        "        if self.training:\n",
        "            future = data.y.view(batch_size, 60, self.output_dim) # (batch, 60, 2)\n",
        "\n",
        "        device = x.device\n",
        "\n",
        "        # Encode past\n",
        "        _, (hidden, cell) = self.encoder(x)\n",
        "\n",
        "        # Initialize decoder input with last observed position\n",
        "        decoder_input = x[:, -1, :self.output_dim].unsqueeze(1)  # (batch, 1, 2)\n",
        "        \n",
        "        # print(\"decoder_input.shape - initial\", decoder_input.shape)  # should be (batch, 1, 2)\n",
        "\n",
        "        outputs = []\n",
        "\n",
        "        for t in range(self.future_steps):\n",
        "            output, (hidden, cell) = self.decoder(decoder_input, (hidden, cell))\n",
        "            pred = self.out(output)  # (batch, 1, 2)\n",
        "            outputs.append(pred)\n",
        "\n",
        "            # TODO: remove forcing ratio?\n",
        "            if self.training and random.random() < forcing_ratio:\n",
        "            # if self.training:\n",
        "                decoder_input = future[:, t].unsqueeze(1)  # ground truth\n",
        "                # print(\"decoder_input.shape - teacher forcing\", decoder_input.shape)  # should be (batch, 1, 2)\n",
        "            else:\n",
        "                decoder_input = pred.detach()  # predicted output as next input\n",
        "                # print(\"decoder_input.shape - autoreg\", decoder_input.shape)  # should be (batch, 1, 2)\n",
        "\n",
        "        outputs = torch.cat(outputs, dim=1)  # (batch, 60, 2)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Example of basic model with simple attention mechanism\n",
        "# class SimpleLSTMWithAttn(nn.Module):\n",
        "#     def __init__(self, input_dim=5, hidden_dim=512, output_dim=60*2):\n",
        "#         super(SimpleLSTMWithAttn, self).__init__()\n",
        "#         self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "        \n",
        "#         # Simple attention mechanism\n",
        "#         self.attention = nn.Linear(hidden_dim, 1)\n",
        "        \n",
        "#         # Add multi-layer prediction head for better results\n",
        "#         self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "#         self.dropout = nn.Dropout(0.1)  # Add dropout for regularization\n",
        "#         self.relu = nn.ReLU()\n",
        "#         self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "#         # Initialize weights properly\n",
        "#         for name, param in self.named_parameters():\n",
        "#             if 'weight' in name:\n",
        "#                 nn.init.xavier_normal_(param)\n",
        "#             elif 'bias' in name:\n",
        "#                 nn.init.constant_(param, 0.0)\n",
        "        \n",
        "#     def forward(self, data):\n",
        "#         x = data.x[..., :5]\n",
        "#         x = x.reshape(-1, 50, 50, 5)  # (batch_size, num_agents, seq_len, input_dim)\n",
        "#         x = x[:, 0, :, :]  # Only consider ego agent (index 0)\n",
        "        \n",
        "#         # Process through LSTM\n",
        "#         lstm_out, _ = self.lstm(x)  # (batch_size, seq_len, hidden_dim)\n",
        "        \n",
        "#         # Apply attention mechanism\n",
        "#         attention_weights = torch.softmax(self.attention(lstm_out), dim=1)  # (batch_size, seq_len, 1)\n",
        "#         attended_features = torch.sum(lstm_out * attention_weights, dim=1)  # (batch_size, hidden_dim)\n",
        "        \n",
        "#         # Process through prediction head\n",
        "#         features = self.relu(self.fc1(attended_features))\n",
        "#         features = self.dropout(features)\n",
        "#         out = self.fc2(features)\n",
        "        \n",
        "#         # Reshape to (batch_size, 60, 2)\n",
        "#         return out.view(-1, 60, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_improved_model(model, train_dataloader, val_dataloader, \n",
        "                         device, criterion=nn.MSELoss(), \n",
        "                         lr=0.001, epochs=150, patience=15, num_agents = 1):\n",
        "    \"\"\"\n",
        "    Improved training function with better debugging and early stopping\n",
        "    \"\"\"\n",
        "    input_dim = model.input_dim # Historical trajectory input dimension\n",
        "    output_dim = model.output_dim  # Future trajectory output dimension\n",
        "    # Initialize optimizer with smaller learning rate\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    \n",
        "    # Exponential decay scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "    \n",
        "    early_stopping_patience = patience\n",
        "    best_val_loss = float('inf')\n",
        "    no_improvement = 0\n",
        "    \n",
        "    # Save initial state for comparison\n",
        "    initial_state_dict = {k: v.clone() for k, v in model.state_dict().items()}\n",
        "    \n",
        "    for epoch in tqdm.tqdm(range(epochs), desc=\"Epoch\", unit=\"epoch\"):\n",
        "        # ---- Training ----\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        num_train_batches = 0\n",
        "        forcing_ratio = max(0.0, 1.0 - epoch / 50)\n",
        "        \n",
        "        for batch in train_dataloader:\n",
        "            batch = batch.to(device)\n",
        "            # print(f\"Batch size: {batch.x.shape}\")\n",
        "            pred = model(batch, forcing_ratio=forcing_ratio).view(-1, 60, output_dim)\n",
        "            # y = batch.y.view(-1, 60, output_dim)\n",
        "            y = batch.y.view(batch.num_graphs, 60, output_dim)\n",
        "            \n",
        "            # Check for NaN predictions\n",
        "            if torch.isnan(pred).any():\n",
        "                print(f\"WARNING: NaN detected in predictions during training\")\n",
        "                continue\n",
        "                \n",
        "            loss = criterion(pred, y)\n",
        "            \n",
        "            # Check if loss is valid\n",
        "            if torch.isnan(loss) or torch.isinf(loss):\n",
        "                print(f\"WARNING: Invalid loss value: {loss.item()}\")\n",
        "                continue\n",
        "                \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            \n",
        "            # More conservative gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            \n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "            num_train_batches += 1\n",
        "        \n",
        "        # Skip epoch if no valid batches\n",
        "        if num_train_batches == 0:\n",
        "            print(\"WARNING: No valid training batches in this epoch\")\n",
        "            continue\n",
        "            \n",
        "        train_loss /= num_train_batches\n",
        "        \n",
        "        # ---- Validation ----\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_mae = 0\n",
        "        val_mse = 0\n",
        "        num_val_batches = 0\n",
        "        \n",
        "        # Sample predictions for debugging\n",
        "        sample_input = None\n",
        "        sample_pred = None\n",
        "        sample_target = None\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch in enumerate(val_dataloader):\n",
        "                batch = batch.to(device)\n",
        "                pred = model(batch)\n",
        "                # pred = pred.view(-1, 50, 60, output_dim)\n",
        "                pred = pred.view(batch.num_graphs, 60, output_dim)\n",
        "                # y = batch.y.view(-1, 50, 60, output_dim)\n",
        "                # y = batch.y.view(-1, 60, output_dim)\n",
        "                y = batch.y.view(batch.num_graphs, 60, 2)\n",
        "                \n",
        "\n",
        "                # Store sample for debugging\n",
        "                if batch_idx == 0 and sample_input is None:\n",
        "                    sample_input = batch.x[0].cpu().numpy()\n",
        "                    sample_pred = pred[0].cpu().numpy()\n",
        "                    sample_target = y[0].cpu().numpy()\n",
        "\n",
        "                # Skip invalid predictions\n",
        "                if torch.isnan(pred).any():\n",
        "                    print(f\"WARNING: NaN detected in predictions during validation\")\n",
        "                    continue\n",
        "                    \n",
        "                # batch_loss = criterion(pred[:, 0, :, :2], y[:, 0, :, :2]).item()\n",
        "                batch_loss = criterion(pred, y).item()\n",
        "                val_loss += batch_loss\n",
        "                \n",
        "\n",
        "                \n",
        "                # pred_unnorm = pred[:, 0, :, :2] * batch.scale.view(-1, 1, 1) + batch.origin.unsqueeze(1)\n",
        "                # y_unnorm = y[:, 0, :, :2] * batch.scale.view(-1, 1, 1) + batch.origin.unsqueeze(1)\n",
        "\n",
        "                # pred_unnorm = pred.view(-1,num_agents,60,output_dim)[:, 0, :, :2] * batch.scale.view(-1, 1, 1) + batch.origin.unsqueeze(1)\n",
        "                # y_unnorm = y.view(-1,num_agents,60,output_dim)[:, 0, :, :2] * batch.scale.view(-1, 1, 1) + batch.origin.unsqueeze(1)\n",
        "                pred_unnorm = pred* batch.scale.view(-1, 1, 1) + batch.origin.unsqueeze(1)\n",
        "                y_unnorm = y * batch.scale.view(-1, 1, 1) + batch.origin.unsqueeze(1)\n",
        "                val_mae += nn.L1Loss()(pred_unnorm, y_unnorm).item()\n",
        "                val_mse += nn.MSELoss()(pred_unnorm, y_unnorm).item()\n",
        "                \n",
        "                num_val_batches += 1\n",
        "        \n",
        "        # Skip epoch if no valid validation batches\n",
        "        if num_val_batches == 0:\n",
        "            print(\"WARNING: No valid validation batches in this epoch\")\n",
        "            continue\n",
        "            \n",
        "        val_loss /= num_val_batches #all agents \n",
        "        val_mae /= num_val_batches\n",
        "        val_mse /= num_val_batches\n",
        "        \n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "        \n",
        "        # Print with more details\n",
        "        tqdm.tqdm.write(\n",
        "            f\"Epoch {epoch:03d} | LR {optimizer.param_groups[0]['lr']:.6f} | \"\n",
        "            f\"Train MSE {train_loss:.4f} | Val MSE {val_loss:.4f} | \"\n",
        "            f\"Val MAE {val_mae:.4f} | Val MSE {val_mse:.4f}\"\n",
        "        )\n",
        "        \n",
        "        # Debug output - first 3 predictions vs targets\n",
        "        if epoch % 5 == 0:\n",
        "            tqdm.tqdm.write(f\"Sample pred first 3 steps: {sample_pred[:3]}\")\n",
        "            tqdm.tqdm.write(f\"Sample target first 3 steps: {sample_target[:3]}\")\n",
        "            tqdm.tqdm.write(f\"Sample input first 3 steps: {sample_input[:3]}\")\n",
        "            \n",
        "            # Check if model weights are changing\n",
        "            if epoch > 0:\n",
        "                weight_change = False\n",
        "                for name, param in model.named_parameters():\n",
        "                    if param.requires_grad:\n",
        "                        initial_param = initial_state_dict[name]\n",
        "                        if not torch.allclose(param, initial_param, rtol=1e-4):\n",
        "                            weight_change = True\n",
        "                            break\n",
        "                if not weight_change:\n",
        "                    tqdm.tqdm.write(\"WARNING: Model weights barely changing!\")\n",
        "        \n",
        "        # Relaxed improvement criterion - consider any improvement\n",
        "        if val_loss < best_val_loss:\n",
        "            tqdm.tqdm.write(f\"Validation improved: {best_val_loss:.6f} -> {val_loss:.6f}\")\n",
        "            best_val_loss = val_loss\n",
        "            no_improvement = 0\n",
        "            torch.save(model.state_dict(), best_model)\n",
        "        else:\n",
        "            no_improvement += 1\n",
        "            if no_improvement >= early_stopping_patience:\n",
        "                print(f\"Early stopping after {epoch+1} epochs without improvement\")\n",
        "                break\n",
        "    \n",
        "    # Load best model before returning\n",
        "    model.load_state_dict(torch.load(best_model))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage\n",
        "def train_and_evaluate_model():\n",
        "    # Create model\n",
        "    model = AutoRegressiveLSTM(input_dim=5, output_dim = 2, hidden_dim=512)\n",
        "    model = model.to(device)\n",
        "    num_agents = 1\n",
        "    # Train with improved function\n",
        "    train_improved_model(\n",
        "        model=model,\n",
        "        train_dataloader=train_dataloader,\n",
        "        val_dataloader=val_dataloader,\n",
        "        device=device,\n",
        "        # lr = 0.007 => 8.946\n",
        "        lr=0.007,  # Lower learning rate\n",
        "        patience=20,  # More patience\n",
        "        epochs=200,\n",
        "        num_agents = num_agents\n",
        "    )\n",
        "    \n",
        "    # Evaluate\n",
        "    model.eval()\n",
        "    test_mse = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            batch = batch.to(device)\n",
        "            pred = model(batch)(-1, num_agents, 60, model.output_dim)\n",
        "            y = batch.y.view(-1, num_agents, 60, model.output_dim)\n",
        "            \n",
        "            # Unnormalize\n",
        "            pred = pred[:, 0, :, :2] * batch.scale.view(-1, 1, 1) + batch.origin.unsqueeze(1)\n",
        "            y = y[:, 0, :, :2] * batch.scale.view(-1, 1, 1) + batch.origin.unsqueeze(1)\n",
        "        \n",
        "            test_mse += nn.MSELoss()(pred, y).item()\n",
        "    \n",
        "    test_mse /= len(val_dataloader)\n",
        "    print(f\"Val MSE: {test_mse:.4f}\")\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch:   0%|          | 0/200 [00:00<?, ?epoch/s]/var/folders/0r/zhth93bs1ygg_ln8t_nhb4f80000gn/T/ipykernel_85741/3794418601.py:32: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  future = future @ R\n",
            "/var/folders/0r/zhth93bs1ygg_ln8t_nhb4f80000gn/T/ipykernel_85741/3794418601.py:34: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  future = future @ R\n",
            "/var/folders/0r/zhth93bs1ygg_ln8t_nhb4f80000gn/T/ipykernel_85741/3794418601.py:45: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  future = future - origin\n",
            "Epoch:   0%|          | 1/200 [00:19<1:05:02, 19.61s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 000 | LR 0.006650 | Train MSE 188314.0890 | Val MSE 11.1665 | Val MAE 18.8698 | Val MSE 547.1597\n",
            "Sample pred first 3 steps: [[-3.1520789  4.1664176]\n",
            " [-1.5467429  5.3311343]\n",
            " [-2.3156488  3.8066468]]\n",
            "Sample target first 3 steps: [[3.2984033e-06 6.9473140e-06]\n",
            " [3.2984033e-06 6.9473140e-06]\n",
            " [3.2984033e-06 6.9473140e-06]]\n",
            "Sample input first 3 steps: [[-3.41748688e-07  1.43109173e-05  6.39591417e-06 -1.12966054e-05\n",
            "   2.33426189e+00]\n",
            " [-2.32525835e-07  1.40127395e-05  6.39591417e-06 -1.12966054e-05\n",
            "   2.33426213e+00]\n",
            " [-1.77852527e-07  1.37678999e-05 -9.72734483e-07 -4.35880975e-06\n",
            "   2.33426237e+00]]\n",
            "Validation improved: inf -> 11.166524\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch:   1%|          | 2/200 [00:42<1:10:22, 21.33s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 001 | LR 0.006317 | Train MSE 108688.3398 | Val MSE 49.0139 | Val MAE 44.1269 | Val MSE 2401.6819\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch:   2%|         | 3/200 [01:05<1:13:39, 22.43s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 002 | LR 0.006002 | Train MSE 70566.8639 | Val MSE 580.0808 | Val MAE 163.6111 | Val MSE 28423.9577\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch:   2%|         | 4/200 [01:28<1:13:59, 22.65s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 003 | LR 0.005702 | Train MSE 54628.8976 | Val MSE 6109.4844 | Val MAE 501.9250 | Val MSE 299364.7266\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch:   2%|         | 5/200 [01:52<1:14:56, 23.06s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 004 | LR 0.005416 | Train MSE 41015.7616 | Val MSE 64.8630 | Val MAE 33.1526 | Val MSE 3178.2854\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch:   3%|         | 6/200 [02:15<1:14:24, 23.01s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 005 | LR 0.005146 | Train MSE 33280.2155 | Val MSE 1430.8133 | Val MAE 249.7088 | Val MSE 70109.8560\n",
            "Sample pred first 3 steps: [[ 0.2755849 15.88982  ]\n",
            " [ 4.6662726  7.496135 ]\n",
            " [ 3.3146532 -3.3866372]]\n",
            "Sample target first 3 steps: [[3.2984033e-06 6.9473140e-06]\n",
            " [3.2984033e-06 6.9473140e-06]\n",
            " [3.2984033e-06 6.9473140e-06]]\n",
            "Sample input first 3 steps: [[-3.41748688e-07  1.43109173e-05  6.39591417e-06 -1.12966054e-05\n",
            "   2.33426189e+00]\n",
            " [-2.32525835e-07  1.40127395e-05  6.39591417e-06 -1.12966054e-05\n",
            "   2.33426213e+00]\n",
            " [-1.77852527e-07  1.37678999e-05 -9.72734483e-07 -4.35880975e-06\n",
            "   2.33426237e+00]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch:   4%|         | 7/200 [02:38<1:13:34, 22.88s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 006 | LR 0.004888 | Train MSE 26366.9200 | Val MSE 17.4167 | Val MAE 21.7402 | Val MSE 853.4182\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch:   4%|         | 8/200 [03:01<1:13:18, 22.91s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 007 | LR 0.004644 | Train MSE 28357.3225 | Val MSE 46.5785 | Val MAE 29.6543 | Val MSE 2282.3475\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch:   4%|         | 9/200 [03:23<1:12:31, 22.78s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 008 | LR 0.004412 | Train MSE 23196.3685 | Val MSE 29.2064 | Val MAE 27.4555 | Val MSE 1431.1125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch:   5%|         | 10/200 [03:46<1:12:25, 22.87s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 009 | LR 0.004191 | Train MSE 22207.2264 | Val MSE 188.5091 | Val MAE 35.2142 | Val MSE 9236.9463\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch:   6%|         | 11/200 [04:11<1:13:40, 23.39s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 010 | LR 0.003982 | Train MSE 19366.6246 | Val MSE 1513.7996 | Val MAE 193.0198 | Val MSE 74176.1801\n",
            "Sample pred first 3 steps: [[ -3.9208276   9.205487 ]\n",
            " [ -2.0843809  13.108226 ]\n",
            " [-19.385746    6.917098 ]]\n",
            "Sample target first 3 steps: [[3.2984033e-06 6.9473140e-06]\n",
            " [3.2984033e-06 6.9473140e-06]\n",
            " [3.2984033e-06 6.9473140e-06]]\n",
            "Sample input first 3 steps: [[-3.41748688e-07  1.43109173e-05  6.39591417e-06 -1.12966054e-05\n",
            "   2.33426189e+00]\n",
            " [-2.32525835e-07  1.40127395e-05  6.39591417e-06 -1.12966054e-05\n",
            "   2.33426213e+00]\n",
            " [-1.77852527e-07  1.37678999e-05 -9.72734483e-07 -4.35880975e-06\n",
            "   2.33426237e+00]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch:   6%|         | 12/200 [04:30<1:09:34, 22.21s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 011 | LR 0.003783 | Train MSE 18155.3653 | Val MSE 52.8365 | Val MAE 33.6065 | Val MSE 2588.9880\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch:   6%|         | 13/200 [04:52<1:08:44, 22.06s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 012 | LR 0.003593 | Train MSE 18339.9084 | Val MSE 613.8317 | Val MAE 88.9525 | Val MSE 30077.7555\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch:   7%|         | 14/200 [05:15<1:09:16, 22.34s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 013 | LR 0.003414 | Train MSE 15277.6654 | Val MSE 386.9674 | Val MAE 99.5486 | Val MSE 18961.4032\n"
          ]
        }
      ],
      "source": [
        "train_and_evaluate_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final Pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_dataset = TrajectoryDatasetTest(test_data, scale=scale)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False,\n",
        "                         collate_fn=lambda xs: Batch.from_data_list(xs))\n",
        "\n",
        "best_model2 = torch.load(best_model)\n",
        "model = AutoRegressiveLSTM(input_dim=5, output_dim = 5, hidden_dim=512).to(device)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.25) # You can try different schedulers\n",
        "# criterion = nn.MSELoss()\n",
        "\n",
        "model.load_state_dict(best_model2)\n",
        "model.eval()\n",
        "\n",
        "pred_list = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        batch = batch.to(device)\n",
        "        pred_norm = model(batch)\n",
        "        \n",
        "        # Reshape the prediction to (N, 60, 2)\n",
        "        pred = pred_norm[...,:2] * batch.scale.view(-1,1,1) + batch.origin.unsqueeze(1)\n",
        "        pred_list.append(pred.cpu().numpy())\n",
        "pred_list = np.concatenate(pred_list, axis=0)  # (N,60,2)\n",
        "pred_output = pred_list.reshape(-1, 2)  # (N*60, 2)\n",
        "output_df = pd.DataFrame(pred_output, columns=['x', 'y'])\n",
        "output_df.index.name = 'index'\n",
        "output_df.to_csv('submission_lstm_5_out.csv', index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>index</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5426.052734</td>\n",
              "      <td>1467.540771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5426.041992</td>\n",
              "      <td>1467.670532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5426.288086</td>\n",
              "      <td>1467.650269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5426.220215</td>\n",
              "      <td>1467.730713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5426.110352</td>\n",
              "      <td>1467.654907</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 x            y\n",
              "index                          \n",
              "0      5426.052734  1467.540771\n",
              "1      5426.041992  1467.670532\n",
              "2      5426.288086  1467.650269\n",
              "3      5426.220215  1467.730713\n",
              "4      5426.110352  1467.654907"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "data-loading-and-submission-preperation",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30918,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "data_science",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 27.524611,
      "end_time": "2025-04-01T17:39:42.223757",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-04-01T17:39:14.699146",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
